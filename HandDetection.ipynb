{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load packages\n",
    "import os\n",
    "import cv2\n",
    "from keras.models import model_from_json\n",
    "from PIL import Image\n",
    "from keras.preprocessing import image\n",
    "from keras.applications.resnet50 import preprocess_input, decode_predictions\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# make sure matplotlib shows images inline\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# set directory\n",
    "os.chdir(\"D:/Documents/GitHub/HandSign_Recognition\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# define the location of the cascade files we will use to detect hands\n",
    "fist_cascade_path = '00 Data/Haarcascades/fist.xml' \n",
    "palm_cascade_path =  '00 Data/Haarcascades/palm.xml'\n",
    "closed_frontal_palm_cascade_path =  '00 Data/Haarcascades/closed_frontal_palm.xml'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# load cascade file\n",
    "fistCascade = cv2.CascadeClassifier(fist_cascade_path)\n",
    "palmCascade = cv2.CascadeClassifier(palm_cascade_path)\n",
    "closedFrontalPalmCascade = cv2.CascadeClassifier(closed_frontal_palm_cascade_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded model from disk\n"
     ]
    }
   ],
   "source": [
    "# load json and create model\n",
    "json_file = open('00 Data/my_model.json', 'r')\n",
    "loaded_model_json = json_file.read()\n",
    "json_file.close()\n",
    "loaded_model = model_from_json(loaded_model_json)\n",
    "\n",
    "# load weights into new model\n",
    "loaded_model.load_weights(\"00 Data/my_model.h5\")\n",
    "print(\"Loaded model from disk\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_dict = pickle.load(open('00 Data/LabelDictionary.pkl', 'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# define the function to predict which letter is shown with handsigns\n",
    "def predict(model, img, target_size):\n",
    "    #    model: keras model\n",
    "    #    img: PIL format image\n",
    "    #    target_size: (width, height) tuple\n",
    "    \n",
    "    if img.size != target_size:\n",
    "        print(\"the original size of the image is: \" + str(img.size))\n",
    "        img = img.resize(target_size)\n",
    "        print(\"the new size of the image is: \" + str(img.size))\n",
    "\n",
    "    # convert to numpy array and preprocess\n",
    "    x = np.array(img)\n",
    "    x = np.expand_dims(x, axis = 0)\n",
    "    x = preprocess_input(x.astype(float))\n",
    "    \n",
    "    # make a prediction\n",
    "    pred = model.predict(x)\n",
    "    #print(pred)\n",
    "    \n",
    "    # label the prediction\n",
    "    pred_label = np.argmax(pred, axis = 1)\n",
    "    #print(pred_label)\n",
    "    \n",
    "    # translate the label to the letter, using the label dictionary\n",
    "    label = list(label_dict.keys())[list(label_dict.values()).index(pred_label)]\n",
    "    \n",
    "    # return the label of the prediction\n",
    "    return label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "camera on: True\n"
     ]
    }
   ],
   "source": [
    "# turn on the camera\n",
    "camera = cv2.VideoCapture(0)\n",
    "\n",
    "# check if the camera is on\n",
    "print(\"camera on: \" + str(camera.isOpened()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fists: 0, palms: 1, front_palms: 0\n",
      "fists: 0, palms: 1, front_palms: 0\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-9-fca039513674>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     13\u001b[0m                                              , minSize = (85, 64), flags = cv2.CASCADE_SCALE_IMAGE)\n\u001b[0;32m     14\u001b[0m         front_palms = closedFrontalPalmCascade.detectMultiScale(image, scaleFactor = 1.1, minNeighbors = 5\n\u001b[1;32m---> 15\u001b[1;33m                                              , minSize = (85, 64), flags = cv2.CASCADE_SCALE_IMAGE)\n\u001b[0m\u001b[0;32m     16\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     17\u001b[0m         \u001b[1;31m# check if there are hands in the image\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# as long as the camera is on, read the images\n",
    "while True:\n",
    "    return_value, image = camera.read()\n",
    "    # check if the camera gives an image\n",
    "    if return_value:\n",
    "        #gray = cv2.cvtColor(image,cv2.COLOR_BGR2GRAY)\n",
    "        cv2.imshow('image', image)\n",
    "\n",
    "        # detect hands\n",
    "        fists = fistCascade.detectMultiScale(image, scaleFactor = 1.1, minNeighbors = 5\n",
    "                                             , minSize = (85, 64), flags = cv2.CASCADE_SCALE_IMAGE)\n",
    "        palms = palmCascade.detectMultiScale(image, scaleFactor = 1.1, minNeighbors = 5\n",
    "                                             , minSize = (85, 64), flags = cv2.CASCADE_SCALE_IMAGE)\n",
    "        front_palms = closedFrontalPalmCascade.detectMultiScale(image, scaleFactor = 1.1, minNeighbors = 5\n",
    "                                             , minSize = (85, 64), flags = cv2.CASCADE_SCALE_IMAGE)\n",
    "\n",
    "        # check if there are hands in the image\n",
    "        if len(fists) > 0 or len(palms) > 0 or len(front_palms) > 0:\n",
    "            \n",
    "            #print(\"fists: \" + str(len(fists)) + \", palms: \" + str(len(palms)) + \", front_palms: \" + str(len(front_palms)))\n",
    "            \n",
    "            # convert opencv image to PIL\n",
    "            pil_im = Image.fromarray(image)\n",
    "            #pil_im.show()\n",
    "            \n",
    "            # try to define the letter from the handsign\n",
    "            new_letter = predict(loaded_model, pil_im, (64,85))\n",
    "            print(new_letter)\n",
    "\n",
    "    if cv2.waitKey(1)& 0xFF == ord('s'):\n",
    "        print ('stop!')\n",
    "        break\n",
    "\n",
    "# turn off the camera\n",
    "camera.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# turn off the camera\n",
    "camera.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
